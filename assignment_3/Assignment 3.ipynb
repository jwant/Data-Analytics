{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant Outliers for the Work Required\n",
    "import sklearn.preprocessing as prep\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up datafram and test data\n",
    "df_test = pd.read_csv(r'test_pub.csv')\n",
    "df = pd.read_csv(r'train.csv').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PREPROCESSING ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0ea8016abf4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Example code from Jun that turns all classifications into binaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# This will remove any unknown entries and assist with using the datas to our advantage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_onehot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Example code from Jun that turns all classifications into binaries\n",
    "# This will remove any unknown entries and assist with using the datas to our advantage\n",
    "df_onehot = pd.get_dummies(df)\n",
    "\n",
    "keys = df_onehot.keys()\n",
    "data_keys = [k for k in keys\n",
    "    if '?' not in k and k[-3:] != \"50K\"]\n",
    "data_train = df_onehot[data_keys]\n",
    "target_train = df_onehot[\"Salary_ >50K\"]\n",
    "\n",
    "df_onehot1 = pd.get_dummies(df_test)\n",
    "# add all zero to non-existing keys\n",
    "for k in data_keys:\n",
    "    if k not in df_onehot1.keys():\n",
    "        df_onehot1[k] = 0\n",
    "\n",
    "data_test = df_onehot1[data_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim Features Used in Testing\n",
    "data_train_features = [i for i in data_train.keys()]\n",
    "\n",
    "# Remove Final Weight and ID, they may be used at a later stage but are being ignored for training\n",
    "# data_train_features.remove('weight')\n",
    "data_train_features.remove('ID')\n",
    "\n",
    "# Since binarisation blows the native country category out we will be keeping only the American natives for training\n",
    "native_keys = [i for i in data_train.keys() if 'Native' in i]\n",
    "native_keys.remove('Native country_ United-States')\n",
    "data_train_features = [i for i in data_train_features if i not in native_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.66579189e-01, 4.56370467e-06, 9.86028078e-01, ...,\n",
       "        1.78579748e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.19207470e-01, 1.76878804e-06, 9.92869367e-01, ...,\n",
       "        4.42197009e-06, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.40164609e-01, 2.08269770e-06, 9.90128215e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [4.38572597e-02, 1.49032888e-06, 9.99037807e-01, ...,\n",
       "        3.11929301e-06, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.50419760e-02, 2.00811115e-06, 9.99385841e-01, ...,\n",
       "        3.92891311e-06, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.32495115e-02, 3.66362823e-06, 9.99912221e-01, ...,\n",
       "        5.49544235e-06, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the descrete functions for data preprocessing for normalisation\n",
    "max_abs_scalar = prep.MaxAbsScaler()\n",
    "min_max_scalar = prep.MinMaxScaler()\n",
    "# normalize = prep.normalize(df)\n",
    "# Normalise Work hours per week\n",
    "# Normalise Capital Gain / Capital Loss\n",
    "\n",
    "col_names = ['Work hours per week','Age','Education years','Capital gain','Capital loss']\n",
    "\n",
    "scaled_features = data_train.copy()\n",
    "features = scaled_features[col_names]\n",
    "scaler = prep.normalize(features.values)\n",
    "# features = scaler.transform(features.values)\n",
    "data_train[col_names] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>weight</th>\n",
       "      <th>Education years</th>\n",
       "      <th>Capital gain</th>\n",
       "      <th>Capital loss</th>\n",
       "      <th>Work hours per week</th>\n",
       "      <th>Employment class_ Federal-gov</th>\n",
       "      <th>Employment class_ Local-gov</th>\n",
       "      <th>Employment class_ Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>Native country_ Portugal</th>\n",
       "      <th>Native country_ Puerto-Rico</th>\n",
       "      <th>Native country_ Scotland</th>\n",
       "      <th>Native country_ South</th>\n",
       "      <th>Native country_ Taiwan</th>\n",
       "      <th>Native country_ Thailand</th>\n",
       "      <th>Native country_ Trinadad&amp;Tobago</th>\n",
       "      <th>Native country_ United-States</th>\n",
       "      <th>Native country_ Vietnam</th>\n",
       "      <th>Native country_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>3.800000e+04</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23987.651500</td>\n",
       "      <td>0.428063</td>\n",
       "      <td>1.893863e+05</td>\n",
       "      <td>0.629365</td>\n",
       "      <td>0.010981</td>\n",
       "      <td>0.020157</td>\n",
       "      <td>0.408919</td>\n",
       "      <td>0.030184</td>\n",
       "      <td>0.065737</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.897132</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13840.226031</td>\n",
       "      <td>0.151622</td>\n",
       "      <td>1.052969e+05</td>\n",
       "      <td>0.159609</td>\n",
       "      <td>0.074783</td>\n",
       "      <td>0.092728</td>\n",
       "      <td>0.124715</td>\n",
       "      <td>0.171096</td>\n",
       "      <td>0.247825</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030765</td>\n",
       "      <td>0.061016</td>\n",
       "      <td>0.021147</td>\n",
       "      <td>0.049411</td>\n",
       "      <td>0.041640</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.048339</td>\n",
       "      <td>0.022355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.188889</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12012.750000</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>1.177460e+05</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24003.500000</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>1.783700e+05</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35947.250000</td>\n",
       "      <td>0.522222</td>\n",
       "      <td>2.363280e+05</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.455435e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID           Age        weight  Education years  \\\n",
       "count  38000.000000  38000.000000  3.800000e+04     38000.000000   \n",
       "mean   23987.651500      0.428063  1.893863e+05         0.629365   \n",
       "std    13840.226031      0.151622  1.052969e+05         0.159609   \n",
       "min        1.000000      0.188889  1.228500e+04         0.062500   \n",
       "25%    12012.750000      0.311111  1.177460e+05         0.562500   \n",
       "50%    24003.500000      0.411111  1.783700e+05         0.625000   \n",
       "75%    35947.250000      0.522222  2.363280e+05         0.750000   \n",
       "max    48000.000000      1.000000  1.455435e+06         1.000000   \n",
       "\n",
       "       Capital gain  Capital loss  Work hours per week  \\\n",
       "count  38000.000000  38000.000000         38000.000000   \n",
       "mean       0.010981      0.020157             0.408919   \n",
       "std        0.074783      0.092728             0.124715   \n",
       "min        0.000000      0.000000             0.010101   \n",
       "25%        0.000000      0.000000             0.404040   \n",
       "50%        0.000000      0.000000             0.404040   \n",
       "75%        0.000000      0.000000             0.454545   \n",
       "max        1.000000      1.000000             1.000000   \n",
       "\n",
       "       Employment class_ Federal-gov  Employment class_ Local-gov  \\\n",
       "count                   38000.000000                 38000.000000   \n",
       "mean                        0.030184                     0.065737   \n",
       "std                         0.171096                     0.247825   \n",
       "min                         0.000000                     0.000000   \n",
       "25%                         0.000000                     0.000000   \n",
       "50%                         0.000000                     0.000000   \n",
       "75%                         0.000000                     0.000000   \n",
       "max                         1.000000                     1.000000   \n",
       "\n",
       "       Employment class_ Never-worked  ...  Native country_ Portugal  \\\n",
       "count                    38000.000000  ...              38000.000000   \n",
       "mean                         0.000211  ...                  0.000947   \n",
       "std                          0.014508  ...                  0.030765   \n",
       "min                          0.000000  ...                  0.000000   \n",
       "25%                          0.000000  ...                  0.000000   \n",
       "50%                          0.000000  ...                  0.000000   \n",
       "75%                          0.000000  ...                  0.000000   \n",
       "max                          1.000000  ...                  1.000000   \n",
       "\n",
       "       Native country_ Puerto-Rico  Native country_ Scotland  \\\n",
       "count                 38000.000000              38000.000000   \n",
       "mean                      0.003737                  0.000447   \n",
       "std                       0.061016                  0.021147   \n",
       "min                       0.000000                  0.000000   \n",
       "25%                       0.000000                  0.000000   \n",
       "50%                       0.000000                  0.000000   \n",
       "75%                       0.000000                  0.000000   \n",
       "max                       1.000000                  1.000000   \n",
       "\n",
       "       Native country_ South  Native country_ Taiwan  \\\n",
       "count           38000.000000            38000.000000   \n",
       "mean                0.002447                0.001737   \n",
       "std                 0.049411                0.041640   \n",
       "min                 0.000000                0.000000   \n",
       "25%                 0.000000                0.000000   \n",
       "50%                 0.000000                0.000000   \n",
       "75%                 0.000000                0.000000   \n",
       "max                 1.000000                1.000000   \n",
       "\n",
       "       Native country_ Thailand  Native country_ Trinadad&Tobago  \\\n",
       "count              38000.000000                     38000.000000   \n",
       "mean                   0.000500                         0.000500   \n",
       "std                    0.022355                         0.022355   \n",
       "min                    0.000000                         0.000000   \n",
       "25%                    0.000000                         0.000000   \n",
       "50%                    0.000000                         0.000000   \n",
       "75%                    0.000000                         0.000000   \n",
       "max                    1.000000                         1.000000   \n",
       "\n",
       "       Native country_ United-States  Native country_ Vietnam  \\\n",
       "count                   38000.000000             38000.000000   \n",
       "mean                        0.897132                 0.002342   \n",
       "std                         0.303791                 0.048339   \n",
       "min                         0.000000                 0.000000   \n",
       "25%                         1.000000                 0.000000   \n",
       "50%                         1.000000                 0.000000   \n",
       "75%                         1.000000                 0.000000   \n",
       "max                         1.000000                 1.000000   \n",
       "\n",
       "       Native country_ Yugoslavia  \n",
       "count                38000.000000  \n",
       "mean                     0.000500  \n",
       "std                      0.022355  \n",
       "min                      0.000000  \n",
       "25%                      0.000000  \n",
       "50%                      0.000000  \n",
       "75%                      0.000000  \n",
       "max                      1.000000  \n",
       "\n",
       "[8 rows x 106 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MODEL TRAINING ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise Data and function\n",
    "from sklearn.ensemble import RandomForestClassifier as rdmfrst\n",
    "\n",
    "feature_list = data_train_features\n",
    "# feature_list = best_feature_scores\n",
    "# fearure_list = [i for i in data_train_features if 'Education leve' in i] # since education did not make the best feats list\n",
    "\n",
    "# recombine data for shuffling\n",
    "forest_data = data_train[feature_list]\n",
    "\n",
    "# split target now that it is shuffled\n",
    "forest_target = target_train\n",
    "\n",
    "n_training_samples = int(len(forest_data) *.80)\n",
    "n_validation_samples = len(forest_data) - n_training_samples\n",
    "\n",
    "forest_train = forest_data.head(n_training_samples)\n",
    "forest_train_target = forest_target.head(n_training_samples)\n",
    "forest_validation = forest_data.tail(n_validation_samples)\n",
    "forest_validation_target = forest_target.tail(n_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "clf = rdmfrst()\n",
    "# clf = rdmfrst(**frst_best_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Model\n",
    "clf.fit(X=forest_train,y=forest_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9494884535230397"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate Model\n",
    "forest_validation_probabilities = clf.predict_proba(forest_validation)[:,1]\n",
    "roc_auc_score(y_true=forest_validation_target, y_score=forest_validation_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Random Forest Parameter Tuning ##\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "distributions = {\n",
    "    'n_estimators': list(range(50,3001,50)),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(1,30)),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True,False],\n",
    "    'class_weight': [None,'balanced', 'balanced_subsample'],\n",
    "}\n",
    "tuning_clf = RandomizedSearchCV(rdmfrst(), distributions, random_state=0)\n",
    "search = tuning_clf.fit(X=forest_train,y=forest_train_target)\n",
    "frst_best_settings = search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Tree Feature Selection ##\n",
    "# Return the key sorted by feature importance\n",
    "def return_feature_importance(keys,clf):\n",
    "    key_importance = []\n",
    "    for i in range(len(keys)):\n",
    "        key_importance += [(keys[i], clf.feature_importances_[i])]\n",
    "    return sorted(key_importance, key=lambda a: a[1], reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [i[0] for i in return_feature_importance(feature_list,clf)[:20]]\n",
    "best_feature_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinations: 364.0\n",
      "('Relationship status_ Husband', 'Marital status_ Never-married', 'Occupation_ Exec-managerial') : 0.9352724024667751\n",
      "('Relationship status_ Husband', 'Occupation_ Exec-managerial', 'Occupation_ Prof-specialty') : 0.9361939623865901\n",
      "('Relationship status_ Husband', 'Occupation_ Exec-managerial', 'Employment class_ Private') : 0.9384896686541747\n",
      "Completed 50 of 364.0\n",
      "('Marital status_ Never-married', 'Occupation_ Exec-managerial', 'Employment class_ Private') : 0.9389324835366148\n",
      "Completed 100 of 364.0\n",
      "('Occupation_ Exec-managerial', 'Occupation_ Prof-specialty', 'Employment class_ Private') : 0.9407788504426114\n",
      "Completed 150 of 364.0\n",
      "('Occupation_ Exec-managerial', 'Employment class_ Private', 'Sex_ Female') : 0.9411236464057615\n",
      "Completed 200 of 364.0\n",
      "Completed 250 of 364.0\n",
      "Completed 300 of 364.0\n",
      "Completed 350 of 364.0\n",
      "('Occupation_ Exec-managerial', 'Employment class_ Private', 'Sex_ Female')\n",
      "0.9411236464057615\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "forest_test_probabilities = clf.predict_proba(data_test[feature_list])[:,1]\n",
    "df_test['Predicted'] = forest_test_probabilities\n",
    "df_test[[\"ID\",\"Predicted\"]].to_csv(\"random_forest_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Support Vector Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "\n",
    "# feature_list = data_train_features\n",
    "fearure_list = best_feature_scores\n",
    "# fearure_list = [i for i in data_train_features if 'Education leve' in i] # since education did not make the best feats list\n",
    "\n",
    "# recombine data for shuffling\n",
    "knn_data = data_train[feature_list]\n",
    "\n",
    "# split target now that it is shuffled\n",
    "knn_target = target_train\n",
    "\n",
    "n_training_samples = int(len(knn_data) *.70)\n",
    "n_validation_samples = len(knn_data) - n_training_samples\n",
    "\n",
    "knn_train = knn_data.head(n_training_samples)\n",
    "knn_train_target = knn_target.head(n_training_samples)\n",
    "knn_validation = knn_data.tail(n_validation_samples)\n",
    "knn_validation_target = knn_target.tail(n_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = knn(n_neighbors=2)\n",
    "clf = knn(**knn_best_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=101, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=knn_train,y=knn_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475245188329601"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_validation_probabilities = clf.predict_proba(knn_validation)[:,1]\n",
    "roc_auc_score(y_true=knn_validation_target, y_score=knn_validation_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K Nearest Neighbour Parameter Tuning ##\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import math\n",
    "\n",
    "distributions = {\n",
    "    'n_neighbors': list(range(1,int(math.sqrt(len(knn_train))),50)),\n",
    "    'weights':['uniform', 'distance'],\n",
    "    'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "tuning_clf = RandomizedSearchCV(knn(), distributions, random_state=0)\n",
    "search = tuning_clf.fit(X=knn_train,y=knn_train_target)\n",
    "knn_best_settings = search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': 'distance', 'n_neighbors': 101, 'algorithm': 'ball_tree'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_best_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "knn_test_probabilities = clf.predict_proba(data_test[feature_list])[:,1]\n",
    "df_test['Predicted'] = knn_test_probabilities\n",
    "df_test[[\"ID\",\"Predicted\"]].to_csv(\"k_nearest_neighbour_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "\n",
    "# feature_list = data_train_features\n",
    "fearure_list = best_feature_scores\n",
    "# fearure_list = [i for i in data_train_features if 'Education leve' in i] # since education did not make the best feats list\n",
    "\n",
    "# recombine data for shuffling\n",
    "lr_data = data_train[feature_list]\n",
    "\n",
    "# split target now that it is shuffled\n",
    "lr_target = target_train\n",
    "\n",
    "n_training_samples = int(len(lr_data) *.70)\n",
    "n_validation_samples = len(lr_data) - n_training_samples\n",
    "\n",
    "lr_train = lr_data.head(n_training_samples)\n",
    "lr_train_target = lr_target.head(n_training_samples)\n",
    "lr_validation = lr_data.tail(n_validation_samples)\n",
    "lr_validation_target = lr_target.tail(n_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = lr(random_state=0,max_iter=2000)\n",
    "clf = lr(**lr_best_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=3000, multi_class='auto', n_jobs=None,\n",
       "                   penalty='none', random_state=None, solver='saga', tol=0.0001,\n",
       "                   verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=lr_train, y=lr_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9038374922791135"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_validation_probabilities = clf.predict_proba(lr_validation)[:,1]\n",
    "roc_auc_score(y_true=lr_validation_target, y_score=lr_validation_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression Parameter Tuning ##\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "distributions = {\n",
    "    'penalty':['l1','l2', 'elasticnet', 'none'],\n",
    "    'dual': [True, False],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': list(range(2000,10000,1000)),\n",
    "}\n",
    "tuning_clf = RandomizedSearchCV(lr(), distributions, random_state=0)\n",
    "search = tuning_clf.fit(X=lr_train,y=lr_train_target)\n",
    "lr_best_settings = search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "lr_test_probabilities = clf.predict_proba(data_test[feature_list])[:,1]\n",
    "df_test['Predicted'] = lr_test_probabilities\n",
    "df_test[[\"ID\",\"Predicted\"]].to_csv(\"logistic_regression_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Combining Predictions ####\n",
    "df_test['Predicted'] = (lr_test_probabilities + knn_test_probabilities + forest_test_probabilities) / 3\n",
    "df_test[[\"ID\",\"Predicted\"]].to_csv(\"combined_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
