{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant Outliers for the Work Required\n",
    "import sklearn.preprocessing as prep\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up datafram and test data\n",
    "df_test = pd.read_csv(r'test-pub.csv')\n",
    "df = pd.read_csv(r'train.csv').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PREPROCESSING ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code from Jun that turns all classifications into binaries\n",
    "# This will remove any unknown entries and assist with using the datas to our advantage\n",
    "df_onehot = pd.get_dummies(df)\n",
    "\n",
    "keys = df_onehot.keys()\n",
    "data_keys = [k for k in keys\n",
    "    if '?' not in k and k[-3:] != \"50K\"]\n",
    "data_train = df_onehot[data_keys]\n",
    "target_train = df_onehot[\"Salary_ >50K\"]\n",
    "\n",
    "df_onehot1 = pd.get_dummies(df_test)\n",
    "# add all zero to non-existing keys\n",
    "for k in data_keys:\n",
    "    if k not in df_onehot1.keys():\n",
    "        df_onehot1[k] = 0\n",
    "\n",
    "data_test = df_onehot1[data_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim Features Used in Testing\n",
    "data_train_features = [i for i in data_train.keys()]\n",
    "\n",
    "# Remove Final Weight and ID, they may be used at a later stage but are being ignored for training\n",
    "data_train_features.remove('ID')\n",
    "data_train_features.remove('Fnlwgt')\n",
    "\n",
    "# Since binarisation blows the native country category out we will be keeping only the American natives for training\n",
    "native_keys = [i for i in data_train.keys() if 'Native' in i]\n",
    "native_keys.remove('Native country_ United-States')\n",
    "data_train_features = [i for i in data_train_features if i not in native_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\pandas\\core\\indexing.py:965: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\pandas\\core\\indexing.py:965: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the descrete functions for data preprocessing for normalisation\n",
    "def normalize_values(dataframe):\n",
    "    max_abs_scalar = prep.MaxAbsScaler()\n",
    "    min_max_scalar = prep.MinMaxScaler()\n",
    "    standard_scalar = prep.StandardScaler()\n",
    "#     robust_scalar = prep.robust_scale()\n",
    "\n",
    "    col_names = ['Work hours per week','Age','Education years','Capital gain','Capital loss']\n",
    "\n",
    "    scaled_features = dataframe.copy()\n",
    "    features = scaled_features[col_names]\n",
    "    scaler = max_abs_scalar.fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    dataframe[col_names] = features\n",
    "    return dataframe\n",
    "\n",
    "data_train = normalize_values(data_train)\n",
    "data_test = normalize_values(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education years</th>\n",
       "      <th>Capital gain</th>\n",
       "      <th>Capital loss</th>\n",
       "      <th>Work hours per week</th>\n",
       "      <th>Employment class_ Federal-gov</th>\n",
       "      <th>Employment class_ Local-gov</th>\n",
       "      <th>Employment class_ Never-worked</th>\n",
       "      <th>Employment class_ Private</th>\n",
       "      <th>Employment class_ Self-emp-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>Relationship status_ Unmarried</th>\n",
       "      <th>Relationship status_ Wife</th>\n",
       "      <th>Race_ Amer-Indian-Eskimo</th>\n",
       "      <th>Race_ Asian-Pac-Islander</th>\n",
       "      <th>Race_ Black</th>\n",
       "      <th>Race_ Other</th>\n",
       "      <th>Race_ White</th>\n",
       "      <th>Sex_ Female</th>\n",
       "      <th>Sex_ Male</th>\n",
       "      <th>Native country_ United-States</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.428134</td>\n",
       "      <td>0.629569</td>\n",
       "      <td>0.009879</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.406152</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.04730</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.662300</td>\n",
       "      <td>0.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.153455</td>\n",
       "      <td>0.161294</td>\n",
       "      <td>0.069901</td>\n",
       "      <td>0.142016</td>\n",
       "      <td>0.124177</td>\n",
       "      <td>0.167534</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4606</td>\n",
       "      <td>0.178122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305665</td>\n",
       "      <td>0.21229</td>\n",
       "      <td>0.093399</td>\n",
       "      <td>0.174673</td>\n",
       "      <td>0.289463</td>\n",
       "      <td>0.093399</td>\n",
       "      <td>0.348451</td>\n",
       "      <td>0.472949</td>\n",
       "      <td>0.472949</td>\n",
       "      <td>0.302795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.188889</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.522222</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age  Education years  Capital gain  Capital loss  \\\n",
       "count  10000.000000     10000.000000  10000.000000  10000.000000   \n",
       "mean       0.428134         0.629569      0.009879      0.030815   \n",
       "std        0.153455         0.161294      0.069901      0.142016   \n",
       "min        0.188889         0.062500      0.000000      0.000000   \n",
       "25%        0.300000         0.562500      0.000000      0.000000   \n",
       "50%        0.411111         0.625000      0.000000      0.000000   \n",
       "75%        0.522222         0.812500      0.000000      0.000000   \n",
       "max        1.000000         1.000000      1.000000      1.000000   \n",
       "\n",
       "       Work hours per week  Employment class_ Federal-gov  \\\n",
       "count         10000.000000                   10000.000000   \n",
       "mean              0.406152                       0.028900   \n",
       "std               0.124177                       0.167534   \n",
       "min               0.010101                       0.000000   \n",
       "25%               0.404040                       0.000000   \n",
       "50%               0.404040                       0.000000   \n",
       "75%               0.454545                       0.000000   \n",
       "max               1.000000                       1.000000   \n",
       "\n",
       "       Employment class_ Local-gov  Employment class_ Never-worked  \\\n",
       "count                 10000.000000                         10000.0   \n",
       "mean                      0.068200                             0.0   \n",
       "std                       0.252101                             0.0   \n",
       "min                       0.000000                             0.0   \n",
       "25%                       0.000000                             0.0   \n",
       "50%                       0.000000                             0.0   \n",
       "75%                       0.000000                             0.0   \n",
       "max                       1.000000                             0.0   \n",
       "\n",
       "       Employment class_ Private  Employment class_ Self-emp-inc  ...  \\\n",
       "count                 10000.0000                    10000.000000  ...   \n",
       "mean                      0.6946                        0.032800  ...   \n",
       "std                       0.4606                        0.178122  ...   \n",
       "min                       0.0000                        0.000000  ...   \n",
       "25%                       0.0000                        0.000000  ...   \n",
       "50%                       1.0000                        0.000000  ...   \n",
       "75%                       1.0000                        0.000000  ...   \n",
       "max                       1.0000                        1.000000  ...   \n",
       "\n",
       "       Relationship status_ Unmarried  Relationship status_ Wife  \\\n",
       "count                    10000.000000                10000.00000   \n",
       "mean                         0.104300                    0.04730   \n",
       "std                          0.305665                    0.21229   \n",
       "min                          0.000000                    0.00000   \n",
       "25%                          0.000000                    0.00000   \n",
       "50%                          0.000000                    0.00000   \n",
       "75%                          0.000000                    0.00000   \n",
       "max                          1.000000                    1.00000   \n",
       "\n",
       "       Race_ Amer-Indian-Eskimo  Race_ Asian-Pac-Islander   Race_ Black  \\\n",
       "count              10000.000000              10000.000000  10000.000000   \n",
       "mean                   0.008800                  0.031500      0.092300   \n",
       "std                    0.093399                  0.174673      0.289463   \n",
       "min                    0.000000                  0.000000      0.000000   \n",
       "25%                    0.000000                  0.000000      0.000000   \n",
       "50%                    0.000000                  0.000000      0.000000   \n",
       "75%                    0.000000                  0.000000      0.000000   \n",
       "max                    1.000000                  1.000000      1.000000   \n",
       "\n",
       "        Race_ Other   Race_ White   Sex_ Female     Sex_ Male  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.008800      0.858600      0.337700      0.662300   \n",
       "std        0.093399      0.348451      0.472949      0.472949   \n",
       "min        0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      1.000000      0.000000      0.000000   \n",
       "50%        0.000000      1.000000      0.000000      1.000000   \n",
       "75%        0.000000      1.000000      1.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       Native country_ United-States  \n",
       "count                   10000.000000  \n",
       "mean                        0.897900  \n",
       "std                         0.302795  \n",
       "min                         0.000000  \n",
       "25%                         1.000000  \n",
       "50%                         1.000000  \n",
       "75%                         1.000000  \n",
       "max                         1.000000  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_train[data_train_features].describe()\n",
    "data_test[data_train_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Classification Class ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier():\n",
    "    features = []\n",
    "    clf = False\n",
    "    clf_model = False\n",
    "    setting_distributions = {}\n",
    "    tuned_settings = {}\n",
    "    train_data = False\n",
    "    train_target = False\n",
    "    validation_data = False\n",
    "    validation_target = False\n",
    "    best_roc_score = 0\n",
    "    best_output = False\n",
    "    \n",
    "    def __init__(self):\n",
    "  \n",
    "        self.features = data_train_features\n",
    "        \n",
    "        data = data_train[self.features]\n",
    "        sample_weight = data_train['Fnlwgt']\n",
    "        data_target = target_train\n",
    "        \n",
    "        n_training_samples = int(len(data) *.80)\n",
    "        n_validation_samples = len(data) - n_training_samples\n",
    "        self.train_data = data.head(n_training_samples)\n",
    "        self.train_target = data_target.head(n_training_samples)\n",
    "        self.train_weight = sample_weight.head(n_training_samples)\n",
    "        self.validation_data = data.tail(n_validation_samples)\n",
    "        self.validation_target = data_target.tail(n_validation_samples)\n",
    "\n",
    "        \n",
    "    def init_model(self,settings=False):\n",
    "        if not self.clf:\n",
    "            return\n",
    "        \n",
    "        if settings:\n",
    "            self.clf_model = self.clf(**settings)\n",
    "        else:\n",
    "            self.clf_model = self.clf()\n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            self.clf_model.fit(X=self.train_data[self.features], y=self.train_target, sample_weight=self.train_weight)\n",
    "        except TypeError:\n",
    "            print('Sample Weight Ignored')\n",
    "            self.clf_model.fit(X=self.train_data[self.features], y=self.train_target)\n",
    "#         todo: calibrate models without predict_proba\n",
    "#         if not hasattr(self.clf_model, 'predict_proba'):\n",
    "#             print('Calibrating model as it does not have the predict_proba attributee')\n",
    "#             calibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
    "#             self.clf_model = calibrated.fit(train, target)\n",
    "#         else:\n",
    "        \n",
    "    def validate(self):\n",
    "        validation_probabilities = self.clf_model.predict_proba(self.validation_data[self.features])[:,1]\n",
    "        score = roc_auc_score(y_true=self.validation_target, y_score=validation_probabilities)\n",
    "        \n",
    "        if score > self.best_roc_score:\n",
    "            self.best_roc_score = score\n",
    "            test_probabilities = self.clf_model.predict_proba(data_test[self.features])[:,1]\n",
    "            self.best_output = df_test.copy()\n",
    "            self.best_output['Predicted'] = test_probabilities\n",
    "            print('Score Improved')\n",
    "        return score\n",
    "    \n",
    "    def export(self, file_name, best=False):\n",
    "        if best:\n",
    "            output = self.best_output\n",
    "        else:\n",
    "            test_probabilities = self.clf_model.predict_proba(data_test[self.features])[:,1]\n",
    "            output = df_test.copy()\n",
    "            output['Predicted'] = test_probabilities\n",
    "            \n",
    "        output[[\"ID\",\"Predicted\"]].to_csv(file_name, index=False)\n",
    "        \n",
    "    def tune_settings(self):            \n",
    "        tuning_clf = RandomizedSearchCV(self.clf(), self.setting_distributions, random_state=0)\n",
    "        search = tuning_clf.fit(X=self.train_data,y=self.train_target)\n",
    "        self.tuned_settings = search.best_params_\n",
    "        return self.tuned_settings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model Training #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Forest Classifier ####\n",
    "from sklearn.ensemble import RandomForestClassifier as rdmfrst\n",
    "rdm_frst_clf = classifier()\n",
    "rdm_frst_clf.clf = rdmfrst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_frst_clf.__init__() # reset data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Improved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9580989761115062"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Validate\n",
    "rdm_frst_clf.init_model(settings=rdm_frst_clf.tuned_settings)\n",
    "rdm_frst_clf.train()\n",
    "rdm_frst_clf.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Settings\n",
    "rdm_frst_clf.setting_distributions = {\n",
    "    'n_estimators': list(range(1000,2000,25)),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(1,100)),\n",
    "    'max_features': ['auto', 'sqrt', 'log2',6],\n",
    "    'min_samples_split': list(range(1,100,10)),\n",
    "    'min_samples_leaf': list(range(1,100,10)),\n",
    "    'bootstrap': [True,False],\n",
    "    'class_weight': [None,'balanced', 'balanced_subsample'],\n",
    "}\n",
    "rdm_frst_clf.tune_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "rdm_frst_clf.export('rdm_frst_v5.csv', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'Capital gain',\n",
       " 'Work hours per week',\n",
       " 'Marital status_ Married-civ-spouse',\n",
       " 'Education years',\n",
       " 'Relationship status_ Husband',\n",
       " 'Capital loss',\n",
       " 'Marital status_ Never-married',\n",
       " 'Occupation_ Exec-managerial',\n",
       " 'Occupation_ Prof-specialty',\n",
       " 'Employment class_ Private',\n",
       " 'Relationship status_ Wife',\n",
       " 'Native country_ United-States',\n",
       " 'Education level_ Bachelors',\n",
       " 'Employment class_ Self-emp-not-inc',\n",
       " 'Sex_ Male',\n",
       " 'Relationship status_ Not-in-family',\n",
       " 'Occupation_ Craft-repair',\n",
       " 'Relationship status_ Own-child',\n",
       " 'Occupation_ Sales']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_feature_importance(keys,clf):\n",
    "    key_importance = []\n",
    "    for i in range(len(keys)):\n",
    "        key_importance += [(keys[i], clf.feature_importances_[i])]\n",
    "    return sorted(key_importance, key=lambda a: a[1], reverse=True)\n",
    "\n",
    "top_tree_features = [i[0] for i in return_feature_importance(rdm_frst_clf.features,rdm_frst_clf.clf_model)[:20]]\n",
    "top_tree_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Nearest Neighbour Classifier ####\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "knn_clf = classifier()\n",
    "knn_clf.clf = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.__init__() # reset data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validate\n",
    "knn_clf.init_model(settings=knn_clf.tuned_settings)\n",
    "knn_clf.train()\n",
    "knn_clf.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Settings\n",
    "import math\n",
    "\n",
    "knn_clf.setting_distributions = {\n",
    "    'n_neighbors': list(range(1,int(math.sqrt(len(knn_clf.train_data))),50)),\n",
    "    'weights':['uniform', 'distance'],\n",
    "    'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': list(range(1,500,50)),\n",
    "    'n_jobs': list(range(1,20)),\n",
    "    'p': [1,2],\n",
    "}\n",
    "knn_clf.tune_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "knn_clf.export('knn_v5.csv', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Logistic Regression Classifier ####\n",
    "from sklearn.linear_model import LogisticRegression as log_reg\n",
    "log_reg_clf = classifier()\n",
    "log_reg_clf.clf = log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf.__init__() # reset data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf.features = top_tree_features # only use top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1505: UserWarning:\n",
      "\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and Validate\n",
    "temp_settings = log_reg_clf.tuned_settings\n",
    "log_reg_clf.init_model(settings=temp_settings)\n",
    "\n",
    "log_reg_clf.train()\n",
    "log_reg_clf.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1505: UserWarning:\n",
      "\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1505: UserWarning:\n",
      "\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1505: UserWarning:\n",
      "\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1505: UserWarning:\n",
      "\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1505: UserWarning:\n",
      "\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning:\n",
      "\n",
      "Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "\n",
      "C:\\Users\\chubb\\Anaconda3\\envs\\Data Anlaytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1505: UserWarning:\n",
      "\n",
      "Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tol': 0.01,\n",
       " 'solver': 'saga',\n",
       " 'penalty': 'none',\n",
       " 'max_iter': 8700,\n",
       " 'fit_intercept': True,\n",
       " 'dual': False,\n",
       " 'class_weight': None,\n",
       " 'C': 0.001}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune Settings\n",
    "log_reg_clf.setting_distributions = {\n",
    "    'C': [0.0001,0.001,0.01,0.1,1,10,100],\n",
    "    'tol':[0.001,0.001,0.01,0.1,1,10,100],\n",
    "    'penalty':['l1','l2', 'elasticnet', 'none'],\n",
    "    'dual': [True, False],\n",
    "    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': list(range(0,10000,100)),\n",
    "    'fit_intercept': [True, False],\n",
    "}\n",
    "log_reg_clf.tune_settings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "log_reg_clf.export('lr_v5.csv', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Support Vector Machine ####\n",
    "from sklearn.svm import SVC\n",
    "svc_clf = classifier()\n",
    "svc_clf.clf = SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.__init__() # reset data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validate\n",
    "temp_settings = svc_clf.tuned_settings\n",
    "# temp_settings['probability'] = True\n",
    "svc_clf.init_model(settings=temp_settings)\n",
    "svc_clf.train()\n",
    "svc_clf.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.setting_distributions = {\n",
    "    'C': [0.001,0.01,0.1,1,10,100],\n",
    "    'kernel':['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "    'degree': list(range(1,10)),\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'coef0': [0.001,0.01,0.1,1,10,100],\n",
    "    'shrinking': [True,False],\n",
    "    'tol': [0.001,0.001,0.01,0.1,1,10,100],\n",
    "    'cache_size': list(range(100,1000,100)),\n",
    "    \n",
    "}\n",
    "svc_clf.tune_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Combine Outputs ####\n",
    "rdm_frst_clf.best_output\n",
    "knn_clf.best_output\n",
    "log_reg_clf.best_output\n",
    "svc_clf.best_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
